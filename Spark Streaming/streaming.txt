[ Section 1 ]
# Apache Storm is a standalone stream processing application
# Spark Streaming-> uses a single machine to do batch processing and stream processing
# In a Spark streaming app, the entire stream of logs is represented as a DStream
# The DStream has a property called batch interval(measured in seconds)
# All data arriving within this interval is grouped into an RDD
# The DStream is a sequence of RDDs; 1 RDD every batch interval

[ Section 2 ]
# First start a stream at localhost using the netcat utility
# > nc -lk 9999        ## spark will start listening for the streaming data at the 9999 port
# >spark-submit Steaming.py localhost 9999    ## run this in another terminal
# Each line went into a separate RDD
# Each line got processed individually
# The count is from a single RDD, not accumulated across RDDs
